{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO7/vPdjrE4l7jeYBgbghWQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrSuicideParrot/HELK/blob/master/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_r0CB11CdSx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c1d01102-136c-4109-dff7-973876d771f5"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from scipy import spatial\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from google.colab import drive\n",
        "import math\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju0WUyjXCojp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Variaveis globais\n",
        "global vocab_size_text\n",
        "global vocab_size_title\n",
        "global MAX_LENGTH_TEXT\n",
        "global MAX_LENGTH_TITLE\n",
        "global mymodel\n",
        "global history\n",
        "global covid_title\n",
        "\n",
        "def readJson(dt):\n",
        "  title = [t[0] for t in list(dt.values())]\n",
        "  text = [t[1] for t in list(dt.values())]\n",
        "  dtk = list(dt.keys())\n",
        "  return pd.DataFrame({'url': dtk, 'title': title, 'text': text})\n",
        "\n",
        "def read(data, json = True):\n",
        "  if json:\n",
        "    df = readJson(data)\n",
        "  else:\n",
        "    df = pd.read_csv(data)\n",
        "  df.dropna(inplace=True)\n",
        "  return df \n",
        "\n",
        "def tokenData(cdata):\n",
        "  cdata = cdata.astype(str)\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "  tokenizer.fit_on_texts(cdata)\n",
        "  word_index = tokenizer.word_index\n",
        "  return tokenizer.texts_to_sequences(cdata), word_index\n",
        "\n",
        "def setVariables(ctext, ctitle):\n",
        "  X_lens = [len(x) for x in ctext.values]\n",
        "  X_lens = np.array(X_lens)\n",
        "\n",
        "  X_lens1 = [len(x) for x in ctitle.values]\n",
        "  X_lens1 = np.array(X_lens1)\n",
        "\n",
        "  m = math.ceil(np.mean(X_lens))\n",
        "  std = math.ceil(np.std(X_lens))\n",
        "  global MAX_LENGTH_TEXT\n",
        "  MAX_LENGTH_TEXT = m + 2*std\n",
        "\n",
        "  m1 = math.ceil(np.mean(X_lens1))\n",
        "  std1 = math.ceil(np.std(X_lens1))\n",
        "  global MAX_LENGTH_TITLE\n",
        "  MAX_LENGTH_TITLE = m1 + 2*std1\n",
        "\n",
        "def splitData(ctext, ctitle, clabel):\n",
        "  split = 0.2\n",
        "  split_n = int(round(len(ctext)*(1-split),0))\n",
        "\n",
        "  train_text = ctext[split_n:]\n",
        "  train_title = ctitle[split_n:]\n",
        "  train_labels = clabel[split_n:]\n",
        "\n",
        "  test_text = ctext[:split_n]\n",
        "  test_title = ctitle[:split_n]\n",
        "  test_labels = clabel[:split_n]\n",
        "\n",
        "  setVariables(ctext, ctitle)\n",
        "  \n",
        "  return train_text, train_title, train_labels, test_text, test_title, test_labels \n",
        "\n",
        "def padding(cdata, title = True): \n",
        "  if title:\n",
        "    maxVar = MAX_LENGTH_TITLE\n",
        "  else:\n",
        "    maxVar = MAX_LENGTH_TEXT\n",
        "    \n",
        "   \n",
        "  return tf.keras.preprocessing.sequence.pad_sequences(cdata,\n",
        "                                              maxlen=maxVar,\n",
        "                                              padding='post',\n",
        "                                              truncating='post')\n",
        "def embedding(word_index, title = True):\n",
        "  fileGlove = '/content/drive/My Drive/fake-news/glove.6B.100d.txt'\n",
        "  embeddings_dict = {}\n",
        "  with open(fileGlove, 'r', encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      embeddings_dict[word] = vector\n",
        "  \n",
        "  if title:\n",
        "    embeddings_matrix = np.zeros((vocab_size_title + 1, 100))\n",
        "  else:\n",
        "    embeddings_matrix = np.zeros((vocab_size_text + 1, 100))\n",
        "    \n",
        "  for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_dict.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embeddings_matrix[i] = embedding_vector\n",
        "  return embeddings_matrix\n",
        "\n",
        "def defineModel(emb_title, emb_text, train_text, train_title, train_labels, test_text, test_title, test_labels):\n",
        "  text_input = tf.keras.layers.Input(shape=(MAX_LENGTH_TEXT,))\n",
        "  text_embed = tf.keras.layers.Embedding(vocab_size_text + 1, 100, weights=[emb_text], trainable=False)(text_input)\n",
        "  text_drop = tf.keras.layers.Dropout(0.2)(text_embed)\n",
        "  text_conv = tf.keras.layers.Conv1D(64, 5, activation='relu')(text_drop)\n",
        "  text_pool = tf.keras.layers.MaxPooling1D(pool_size=4)(text_conv)\n",
        "  text_lstm1 = tf.keras.layers.LSTM(20, return_sequences=True)(text_pool)\n",
        "  text_lstm2 = tf.keras.layers.LSTM(20)(text_lstm1)\n",
        "\n",
        "  title_input = tf.keras.layers.Input(shape=(MAX_LENGTH_TITLE,))\n",
        "  title_embed = tf.keras.layers.Embedding(vocab_size_title + 1, 100, weights=[emb_title], trainable=False)(title_input)\n",
        "  title_drop = tf.keras.layers.Dropout(0.2)(title_embed)\n",
        "  title_conv = tf.keras.layers.Conv1D(64, 5, activation='relu')(title_drop)\n",
        "  title_pool = tf.keras.layers.MaxPooling1D(pool_size=4)(title_conv)\n",
        "  title_lstm1 = tf.keras.layers.LSTM(20, return_sequences=True)(title_pool)\n",
        "  title_lstm2 = tf.keras.layers.LSTM(20)(title_lstm1)\n",
        "\n",
        "  concat = tf.keras.layers.concatenate([text_lstm2, title_lstm2])\n",
        "\n",
        "  drop = tf.keras.layers.Dropout(0.2)(concat)\n",
        "  dense1 = tf.keras.layers.Dense(100)(drop)\n",
        "  drop1 = tf.keras.layers.Dropout(0.3)(dense1)\n",
        "  dense2 = tf.keras.layers.Dense(50)(drop1)\n",
        "  output = tf.keras.layers.Dense(1, activation='sigmoid')(drop1)\n",
        "\n",
        "  model = tf.keras.models.Model(inputs=[text_input, title_input], outputs=output)\n",
        "  #model.summary()\n",
        "\n",
        "  METRICS = [\n",
        "        tf.metrics.Precision(name='precision'),\n",
        "        tf.metrics.Recall(name='recall'),\n",
        "        tf.metrics.AUC(name='auc'),\n",
        "  ]\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(0.0005),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=METRICS)\n",
        "  \n",
        "  callback = tf.keras.callbacks.EarlyStopping(monitor='val_auc',\n",
        "                                            patience=5,\n",
        "                                            mode='max',\n",
        "                                          restore_best_weights=True)\n",
        "  history = model.fit([train_text, train_title],\n",
        "                     train_labels,\n",
        "                     epochs=5,\n",
        "                     batch_size=100,\n",
        "                     validation_data=(\n",
        "                           [test_text, test_title],\n",
        "                            test_labels\n",
        "                     ),\n",
        "  callbacks=[callback])\n",
        "\n",
        "  return model, history\n",
        "\n",
        "def prediction(test):\n",
        "  preds = mymodel.predict(test)\n",
        "  return [p[0]for p in preds]\n",
        "\n",
        "def covidNews(path):\n",
        "  covid = read(path, json = False)\n",
        "  covid = covid[:500]\n",
        "  covid['title'],_ = tokenData(covid['title'])\n",
        "  global covid_title\n",
        "  covid_title = padding(covid['title'], title = True)\n",
        "\n",
        "def avg(x):\n",
        "  return np.mean(list(filter(lambda y : y > 0, x)))\n",
        "\n",
        "def comparisons(test_title):\n",
        "    return [avg(list(map(lambda y: spatial.distance.cosine(x, y), covid_title))) for x in test_title]\n",
        "\n",
        "def trainModel(path, debug = False):\n",
        "  df = read(path, json = False)\n",
        "\n",
        "  if debug:\n",
        "    print(\"Read data frame\")\n",
        "\n",
        "  df['title'], word_index_title = tokenData(df['title'])\n",
        "  global vocab_size_title\n",
        "  vocab_size_title = len(word_index_title)\n",
        "  df['text'], word_index_text = tokenData(df['text'])\n",
        "  global vocab_size_text\n",
        "  vocab_size_text = len(word_index_text)\n",
        "\n",
        "  if debug:\n",
        "    print(\"Tokenizer complete\")\n",
        "\n",
        "  train_text, train_title, train_labels, test_text, test_title, test_labels = splitData(df['text'], df['title'], df['label'])\n",
        "\n",
        "  train_text = padding(train_text, title = False)\n",
        "  train_title = padding(train_title, title = True)\n",
        "  test_title = padding(test_title, title = True)\n",
        "  test_text = padding(test_text, title = False)\n",
        "  \n",
        "  if debug:\n",
        "    print(\"Split and padding done\")\n",
        "\n",
        "  embeddings_matrix_title = embedding(word_index_title, title = True)\n",
        "  embeddings_matrix_text = embedding(word_index_text, title = False)\n",
        "\n",
        "  if debug:\n",
        "    print(\"Embeddings all set\")\n",
        "\n",
        "  global mymodel, history\n",
        "  mymodel, history = defineModel(embeddings_matrix_title, embeddings_matrix_text, train_text, train_title, train_labels, test_text, test_title, test_labels)\n",
        "  \n",
        "  if debug:\n",
        "    print(\"Model compiled\")\n",
        "  \n",
        "  !mkdir -p /content/drive/My\\ Drive/saved_model\n",
        "  covidNews(\"/content/drive/My Drive/fake-news/corona_dataset.csv\")\n",
        "  np.save('/content/drive/My Drive/saved_model/covid_title.npy', covid_title)\n",
        "\n",
        "  if debug:\n",
        "    print(\"Covid comparisons\")\n",
        "\n",
        "  mymodel.save('/content/drive/My Drive/saved_model/my_model')\n",
        "\n",
        "def testModel(path, debug = False):\n",
        "  try:\n",
        "    mymodel\n",
        "  except NameError:\n",
        "    global model\n",
        "    mymodel = tf.keras.models.load_model('/content/drive/My Drive/saved_model/my_model')\n",
        "    global covid_title\n",
        "    covid_title = np.load('/content/drive/My Drive/saved_model/covid_title.npy')\n",
        "  \n",
        "  df = read(path, json = True)\n",
        "  \n",
        "  if debug:\n",
        "    print(\"Data frame read from json\")\n",
        "\n",
        "  df['title'], word_index_title = tokenData(df['title'])\n",
        "  global vocab_size_title\n",
        "  vocab_size_title = len(word_index_title)\n",
        "  df['text'], word_index_text = tokenData(df['text'])\n",
        "  global vocab_size_text\n",
        "  vocab_size_text = len(word_index_text)\n",
        "  \n",
        "  if debug:\n",
        "    print(\"Tokenizer complete\")\n",
        "\n",
        "  test_title = padding(df['title'], title = True)\n",
        "  test_text = padding(df['text'], title = False)\n",
        "\n",
        "  if debug:\n",
        "    print(\"Padding done\")\n",
        "  \n",
        "  preds = prediction([test_text, test_title])\n",
        "\n",
        "  if debug:\n",
        "    print(\"Predictions\")\n",
        "\n",
        "  t = comparisons(test_title)\n",
        "\n",
        "  data = {}\n",
        "  data['url'] = df['url']\n",
        "  data['predictions'] = preds\n",
        "  data['compare'] = t\n",
        "  \n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbQxaOABCtVA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "5f50bd87-bd9f-4259-b71b-f1cbf37437c4"
      },
      "source": [
        "export = trainModel(path = '/content/drive/My Drive/fake-news/train.csv', debug = True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read data frame\n",
            "Tokenizer complete\n",
            "Split and padding done\n",
            "Embeddings all set\n",
            "Epoch 1/5\n",
            "37/37 [==============================] - 11s 306ms/step - loss: 0.6696 - precision: 0.5217 - recall: 0.0961 - auc: 0.6243 - val_loss: 0.6158 - val_precision: 0.8825 - val_recall: 0.2254 - val_auc: 0.8938\n",
            "Epoch 2/5\n",
            "37/37 [==============================] - 10s 265ms/step - loss: 0.4815 - precision: 0.7864 - recall: 0.7123 - auc: 0.8815 - val_loss: 0.3395 - val_precision: 0.7711 - val_recall: 0.9532 - val_auc: 0.9268\n",
            "Epoch 3/5\n",
            "37/37 [==============================] - 10s 263ms/step - loss: 0.2970 - precision: 0.8155 - recall: 0.9316 - auc: 0.9354 - val_loss: 0.2613 - val_precision: 0.8199 - val_recall: 0.9552 - val_auc: 0.9470\n",
            "Epoch 4/5\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 0.2547 - precision: 0.8330 - recall: 0.9464 - auc: 0.9508 - val_loss: 0.2440 - val_precision: 0.8353 - val_recall: 0.9379 - val_auc: 0.9538\n",
            "Epoch 5/5\n",
            "37/37 [==============================] - 10s 264ms/step - loss: 0.2259 - precision: 0.8556 - recall: 0.9458 - auc: 0.9640 - val_loss: 0.2334 - val_precision: 0.8505 - val_recall: 0.9235 - val_auc: 0.9591\n",
            "Model compiled\n",
            "Covid comparisons\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}